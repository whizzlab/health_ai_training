{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c67812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import datetime\n",
    "\n",
    "#for BERT\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b49073-2f62-4ca1-9e42-e55654107c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU options to limit OOM erors\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa195a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure csv only contains labelled examples\n",
    "#in excel, have concat title and abstract into same text block = feature. \n",
    "labelled = pd.read_csv('training_data/training_maturity.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f78b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisdf = labelled[['include', 'feature']].copy()       \n",
    "                      \n",
    "#pipeline now independent from csv\n",
    "analysisdf['include'] = analysisdf['include'].astype(np.int64)\n",
    "analysisdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c39b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Significant imbalance -> but apparently not too much of a problem when using BERT? \n",
    "#Could try stratified k-folds in future iterations?\n",
    "\n",
    "print(\"labels:\")\n",
    "print(analysisdf['include'].value_counts())\n",
    "\n",
    "plt.figure()\n",
    "pd.value_counts(analysisdf['include']).plot.bar(title=\"inclusion 0 vs 1\")\n",
    "plt.xlabel(\"inclusion\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisdf[analysisdf['feature'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb129b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysisdf.dropna(subset=['feature'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d047fb",
   "metadata": {},
   "source": [
    "## creating training/validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55685ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tempdf, valdf = train_test_split(analysisdf, test_size=0.1, stratify=analysisdf['include'])\n",
    "\n",
    "print(\"tempdf\")\n",
    "plt.figure()\n",
    "pd.value_counts(tempdf['include']).plot.bar(title=\"inclusion 0 vs 1\")\n",
    "plt.xlabel(\"inclusion\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.show()\n",
    "\n",
    "print(\"valdf\")\n",
    "plt.figure()\n",
    "pd.value_counts(valdf['include']).plot.bar(title=\"inclusion 0 vs 1\")\n",
    "plt.xlabel(\"inclusion\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf, testdf = train_test_split(tempdf, test_size=0.12, stratify=tempdf['include'])\n",
    "\n",
    "print(\"traindf\")\n",
    "plt.figure()\n",
    "pd.value_counts(traindf['include']).plot.bar(title=\"inclusion 0 vs 1\")\n",
    "plt.xlabel(\"inclusion\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.show()\n",
    "\n",
    "print(\"testdf\")\n",
    "plt.figure()\n",
    "pd.value_counts(testdf['include']).plot.bar(title=\"inclusion 0 vs 1\")\n",
    "plt.xlabel(\"inclusion\")\n",
    "plt.ylabel(\"number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec566f88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "traindf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6dd023",
   "metadata": {},
   "source": [
    "## Putting dataframes into tensor wrappers\n",
    "\n",
    "https://www.tensorflow.org/guide/data\n",
    "\n",
    "from_tensor_slices creates a tensor wrapper that combines training features with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c979686",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRAINING WRAPPER\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((traindf['feature'].to_numpy().reshape(-1,1),\n",
    "                                               traindf['include'].to_numpy().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##VALIDATION WRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices((valdf['feature'].to_numpy().reshape(-1,1),\n",
    "                                             valdf['include'].to_numpy().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e60ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEST SET WRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d07f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((testdf['feature'].to_numpy().reshape(-1,1),\n",
    "                                              testdf['include'].to_numpy().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now have three tensor data wrappers - train_ds and val_ds that can be fed into tf pipe, and test_ds for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2c746-58c3-4af7-a0f2-9a6d0396f823",
   "metadata": {},
   "source": [
    "## Batch and optimise datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695aeb6-8d01-421c-8145-c69d8021ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715291c4-dadf-4f48-8099-5cf2a0fd47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_ds_batched = configure_for_performance(train_ds)\n",
    "val_ds_batched = configure_for_performance(val_ds)\n",
    "test_ds_batched = configure_for_performance(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3210971b",
   "metadata": {},
   "source": [
    "## Configuring BERT: 1) preprocessing 2) vectorization\n",
    "\n",
    "Code is largely adapted from here (also has links to other BERT libraries): https://www.tensorflow.org/text/tutorials/classify_text_with_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea148a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT model for vectorization\n",
    "\n",
    "bert_vec_model = 'experts_pubmed'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_vec_model]\n",
    "\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder) #wraps this as a Keras layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT model for pre-processing\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_vec_model]\n",
    "\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess) #wraps this as a Keras layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'BERT model selected: {tfhub_handle_encoder}')\n",
    "print(f'Pre-process model selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52802f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check bert_preprocess_model on test text - preprocessing should split tokens into word ids / mask /type\n",
    "\n",
    "text_test = ['This artificial intelligence model predicts cardiovascular risk from echocardiogram images']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b09b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check bert_model is functional for given inputs\n",
    "\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a243c5",
   "metadata": {},
   "source": [
    "## Building BERT classifier pipeline\n",
    "\n",
    "This pipeline will: take raw data wrappers -> pre-process and encode with BERT (using Keraslayers defined above) -> classify using simple net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(seq_length=512):\n",
    "\n",
    "    # Define input layer\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    \n",
    "    # Load the pretrained preprocessor\n",
    "    bert_preprocessor = hub.load(tfhub_handle_preprocess)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    tokenizer = hub.KerasLayer(bert_preprocessor.tokenize, name='tokenizer')\n",
    "    tokenized_inputs = [tokenizer(text_input)]\n",
    "\n",
    "    # Pack the tokenized input for the encoder\n",
    "    bert_pack_inputs = hub.KerasLayer(bert_preprocessor.bert_pack_inputs,\n",
    "                                      arguments=dict(seq_length=seq_length), name='packer')\n",
    "    \n",
    "    encoder_inputs = bert_pack_inputs(tokenized_inputs)\n",
    "    \n",
    "    #BERT encoding layer\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    \n",
    "    #Output layers\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "\n",
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ccc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "#metrics\n",
    "#metrics = ['BinaryAccuracy']\n",
    "#metrics = ['accuracy']\n",
    "\n",
    "metrics = tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, threshold=0.5)\n",
    "\n",
    "#epochs\n",
    "epochs = 3\n",
    "\n",
    "#optimization\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds_batched).numpy()\n",
    "print(steps_per_epoch)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "print(num_train_steps)\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "print(num_warmup_steps)\n",
    "init_lr = 1e-5\n",
    "\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "\n",
    "\n",
    "# models.optimization dependent on tf-official-models, which depends on pycococo which does not install on windows without workaround:\n",
    "# https://github.com/philferriere/cocoapi\n",
    "# 1) upgrade visual basic to 2019 and install C++ tools in that library\n",
    "# 2) install pycococo using direct from git installation in the github link (may need git library first)\n",
    "# 3) then install tf-official-models using pip\n",
    "\n",
    "##alternative optimizer\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "#                                        name='Adam'\n",
    "#                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f95b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64560f20",
   "metadata": {},
   "source": [
    "## Fit classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds_batched,\n",
    "                               validation_data=val_ds_batched,\n",
    "                               epochs=epochs\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c8de2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##re-test on test_ds\n",
    "\n",
    "loss, binary_accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {binary_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc66591-4490-4b6a-bdd7-f30057f50bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_model.predict(testdf['feature'])\n",
    "y_pred = y_pred.reshape(463)\n",
    "y_pred = np.asarray(tf.round(tf.nn.sigmoid(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c603d-254f-4121-bd73-a6e8b4a5f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf9f363-e210-43ba-8d3b-761f214856d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(testdf['include'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6df07c-15bc-4d67-92a3-d1b0e47b13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = ConfusionMatrixDisplay(confusion_matrix(testdf['include'], y_pred))\n",
    "\n",
    "cm.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651328d-4910-403e-87da-d7bdb5738719",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.loc[:,'predicted'] = np.asarray(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6668f0-9ee2-4c8f-9d2b-baa825b4cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(y_pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c0ae5-2c79-492f-9b44-55b244798721",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = testdf[testdf.include != testdf.predicted]\n",
    "misclassified.to_csv('bert_misclassified_maturity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b5e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finish_early as now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d488f8ca",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPORT MODEL\n",
    "\n",
    "saved_model_path = 'models/maturity_bert'\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb7c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801474d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
